# Project: Data Warehouse




## Introduction

The project aims at organizing a large number of JSON files in order to allow the analytics team to draw conclusions
about the users' usage of the service "Sparkify." Two sets of JSON files can be distinguished. The first set of the JSON files contains information
about songs, such as artist name, title, duration of the song, and so on. The second dataset describes log sessions 
of Sparkify users. In the logs, we can find data about the usage of the services and which songs have been played for 
how long and how often. As the data is unorganized and it is difficult to draw valuable insights for the analytics team.
Thus, we decided to use the dimensional modeling to create a star schema for our data.


## General outline
The  process can be divided into the following step:
1. We create empty tables for our schema.
2. We load data from s3 buckets to staging tables in the Redshift database.
3. We extract data from these staging tables in order to create a star schema, which is optimal for the analytics teams'
goals.


## The star schema of the database

The star schema contains the following tables:

* songplays - stands for the fact table containing the information about song plays.
* users - stands for the dimension table containing the information about song users of the app
* songs - stands for the dimension table containing the information about songs found in the app
* artist - stands for the dimension table containing the information about artits found in the app  
* time - stands for the dimension table containing the information about timestamps of records in the songsplay


## Usage
The scripts work properly if we create a Redshift cluster with an associated IAM ROLE(the role can read S3 buckets files)
and a security group that defines an endpoint that we can connect to.
In order to create schema and load populate table with data, one has to execute scripts in the following order:

1. create_tables.py
2. etl.py

The script _sql_queries.py_ contains sql queries that enable to create tables and populate them with the data.

## Dependecies

We need to pip install psycopg2 Python library. The recommended Python version is 3.7 or above.

## Author

by Teodor Wi≈õniewski